{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "from modules.datasets import MultiLabelDataset,ContrastiveDataset\n",
    "from modules.siamese import SiameseNetwork\n",
    "import torchdatasets as td\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\") # we want to run Single-Core -> Ignore this warning\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13809"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MultiLabelDataset(\"./Carparts\",cache_in_ram=True,cache_path=\"./cache\")\n",
    "contrastiveDataset = ContrastiveDataset(dataset)\n",
    "len(contrastiveDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(contrastiveDataset))\n",
    "test_size = len(contrastiveDataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(contrastiveDataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                  [-1, 512]       1,049,088\n",
      "            ReLU-175                  [-1, 512]               0\n",
      "          Linear-176                  [-1, 256]         131,328\n",
      "            ReLU-177                  [-1, 256]               0\n",
      "          Linear-178                  [-1, 128]          32,896\n",
      "================================================================\n",
      "Total params: 24,721,344\n",
      "Trainable params: 24,721,344\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 94.30\n",
      "Estimated Total Size (MB): 381.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(256, 128),\n",
    ")\n",
    "summary(model.to(DEVICE), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | model     | ResNet          | 24.7 M\n",
      "1 | criterion | ContrastiveLoss | 0     \n",
      "----------------------------------------------\n",
      "24.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.7 M    Total params\n",
      "98.885    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 109/109 [11:55<00:00,  6.57s/it, loss=0.255, v_num=4]  \n"
     ]
    }
   ],
   "source": [
    "siamese_model = SiameseNetwork(model)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=5, precision=\"bf16\",benchmark=True,fast_dev_run=False)\n",
    "trainer.fit(model=siamese_model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3259, -1.0109, -0.2502,  0.7671,  0.0248, -0.2950,  0.4633, -0.1364,\n",
      "          0.2257,  0.0041, -0.3134, -0.0305,  0.4244,  0.0252,  0.0759,  0.0993,\n",
      "          0.8282,  0.0491, -0.1028, -0.6206,  0.4152, -0.0645, -0.3571,  0.8471,\n",
      "         -0.0549, -0.0816,  0.3380, -0.3665,  0.5714,  0.5733, -0.6821, -0.2083,\n",
      "         -0.3417, -0.0619,  0.9279,  0.4821,  0.1775, -0.1955,  0.3373,  0.3271,\n",
      "          0.1928, -0.1052,  0.1878,  0.1877,  0.1110, -0.2217,  0.3067, -0.7441,\n",
      "         -1.2032,  1.1072, -0.1086, -0.5298, -0.2465, -0.1587, -0.0512, -0.7199,\n",
      "         -0.0470,  0.0708, -0.6328, -0.0833,  0.6278,  0.3888, -0.1535, -0.2640,\n",
      "         -0.4642, -0.4565, -0.1586, -0.2788,  0.6138,  0.0537, -0.6038,  0.4900,\n",
      "         -0.6109, -0.5883,  0.1327,  0.8794, -0.2659, -0.3363,  0.4326, -0.0743,\n",
      "          0.0701,  0.4669, -0.2329, -1.0083, -0.5487, -0.3766,  0.3819,  0.6669,\n",
      "         -0.2136,  0.2426, -0.2461, -0.4134, -0.1024, -0.4160, -0.3675, -0.4726,\n",
      "          0.6631,  0.3613, -0.3361, -0.5397,  0.9267, -0.0476,  0.9696, -0.0921,\n",
      "          0.3011, -0.2580,  0.8037,  0.2124, -0.5572,  0.1241,  0.0775, -0.0566,\n",
      "          0.6894, -0.5726, -0.0147, -0.0741, -0.0420, -0.0979,  0.5914, -0.1616,\n",
      "         -0.2711,  0.2262, -0.6123, -0.5622,  0.8059, -0.0678,  0.1851,  0.6844]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3176, -0.9869, -0.2396,  0.7360,  0.0296, -0.2819,  0.4558, -0.1444,\n",
      "          0.2221,  0.0063, -0.3060, -0.0360,  0.4212,  0.0153,  0.0764,  0.1114,\n",
      "          0.8202,  0.0456, -0.1071, -0.6210,  0.4085, -0.0783, -0.3362,  0.8479,\n",
      "         -0.0585, -0.0837,  0.3302, -0.3548,  0.5641,  0.5713, -0.6861, -0.2063,\n",
      "         -0.3228, -0.0557,  0.9232,  0.4680,  0.1791, -0.2067,  0.3413,  0.3226,\n",
      "          0.1867, -0.1046,  0.1821,  0.1691,  0.1095, -0.2290,  0.3015, -0.7498,\n",
      "         -1.1699,  1.0880, -0.1178, -0.5079, -0.2387, -0.1520, -0.0501, -0.7152,\n",
      "         -0.0355,  0.0665, -0.6140, -0.0688,  0.6156,  0.3918, -0.1542, -0.2743,\n",
      "         -0.4754, -0.4671, -0.1475, -0.2733,  0.6119,  0.0414, -0.6031,  0.4833,\n",
      "         -0.5915, -0.5796,  0.1423,  0.8725, -0.2535, -0.3280,  0.4233, -0.0598,\n",
      "          0.0568,  0.4662, -0.2348, -0.9899, -0.5398, -0.3875,  0.3598,  0.6552,\n",
      "         -0.2144,  0.2472, -0.2564, -0.3974, -0.0844, -0.4234, -0.3568, -0.4791,\n",
      "          0.6464,  0.3470, -0.3341, -0.5222,  0.9360, -0.0263,  0.9458, -0.0779,\n",
      "          0.2961, -0.2570,  0.7867,  0.2011, -0.5548,  0.1266,  0.0952, -0.0582,\n",
      "          0.6956, -0.5575, -0.0134, -0.0706, -0.0567, -0.0852,  0.5936, -0.1646,\n",
      "         -0.2754,  0.2172, -0.5894, -0.5381,  0.8032, -0.0672,  0.1978,  0.6824]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3222, -0.9855, -0.2415,  0.7424,  0.0308, -0.2822,  0.4573, -0.1365,\n",
      "          0.2258,  0.0083, -0.3086, -0.0391,  0.4203,  0.0169,  0.0823,  0.1076,\n",
      "          0.8172,  0.0501, -0.1089, -0.6152,  0.4095, -0.0794, -0.3417,  0.8509,\n",
      "         -0.0532, -0.0848,  0.3325, -0.3524,  0.5645,  0.5725, -0.6850, -0.2083,\n",
      "         -0.3206, -0.0556,  0.9143,  0.4715,  0.1738, -0.1961,  0.3407,  0.3224,\n",
      "          0.1895, -0.1084,  0.1908,  0.1665,  0.1033, -0.2251,  0.3016, -0.7485,\n",
      "         -1.1752,  1.0892, -0.1150, -0.5138, -0.2470, -0.1494, -0.0439, -0.7088,\n",
      "         -0.0336,  0.0588, -0.6126, -0.0682,  0.6139,  0.3892, -0.1607, -0.2702,\n",
      "         -0.4712, -0.4685, -0.1515, -0.2776,  0.6046,  0.0495, -0.6037,  0.4812,\n",
      "         -0.5991, -0.5809,  0.1421,  0.8715, -0.2538, -0.3260,  0.4220, -0.0541,\n",
      "          0.0645,  0.4661, -0.2345, -0.9904, -0.5473, -0.3819,  0.3580,  0.6585,\n",
      "         -0.2105,  0.2392, -0.2582, -0.4032, -0.0916, -0.4232, -0.3573, -0.4747,\n",
      "          0.6400,  0.3545, -0.3336, -0.5284,  0.9313, -0.0291,  0.9566, -0.0813,\n",
      "          0.2975, -0.2554,  0.7944,  0.2008, -0.5510,  0.1213,  0.0905, -0.0508,\n",
      "          0.7006, -0.5575, -0.0133, -0.0672, -0.0591, -0.0902,  0.5823, -0.1592,\n",
      "         -0.2808,  0.2248, -0.5898, -0.5459,  0.8017, -0.0722,  0.1909,  0.6737]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3254, -1.0008, -0.2402,  0.7542,  0.0271, -0.2886,  0.4625, -0.1311,\n",
      "          0.2201,  0.0069, -0.3018, -0.0280,  0.4244,  0.0268,  0.0719,  0.0984,\n",
      "          0.8211,  0.0451, -0.1144, -0.6262,  0.4172, -0.0694, -0.3492,  0.8428,\n",
      "         -0.0611, -0.0740,  0.3398, -0.3574,  0.5728,  0.5681, -0.6779, -0.2188,\n",
      "         -0.3268, -0.0548,  0.9144,  0.4739,  0.1724, -0.2081,  0.3430,  0.3249,\n",
      "          0.1963, -0.0974,  0.1916,  0.1831,  0.1125, -0.2182,  0.3058, -0.7459,\n",
      "         -1.1886,  1.0898, -0.1113, -0.5265, -0.2387, -0.1459, -0.0550, -0.7091,\n",
      "         -0.0396,  0.0713, -0.6169, -0.0792,  0.6204,  0.3886, -0.1641, -0.2559,\n",
      "         -0.4759, -0.4576, -0.1435, -0.2731,  0.6092,  0.0411, -0.6053,  0.4861,\n",
      "         -0.6014, -0.5951,  0.1354,  0.8692, -0.2607, -0.3252,  0.4266, -0.0758,\n",
      "          0.0682,  0.4702, -0.2413, -1.0000, -0.5412, -0.3831,  0.3654,  0.6526,\n",
      "         -0.2057,  0.2312, -0.2513, -0.4109, -0.0941, -0.4184, -0.3616, -0.4662,\n",
      "          0.6506,  0.3565, -0.3323, -0.5351,  0.9154, -0.0404,  0.9568, -0.0881,\n",
      "          0.2972, -0.2541,  0.7884,  0.1958, -0.5492,  0.1224,  0.0842, -0.0614,\n",
      "          0.6863, -0.5722, -0.0089, -0.0632, -0.0429, -0.0897,  0.5745, -0.1534,\n",
      "         -0.2746,  0.2208, -0.5920, -0.5555,  0.7913, -0.0660,  0.1937,  0.6762]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.2121e-01, -9.9937e-01, -2.4792e-01,  7.6230e-01,  3.7028e-02,\n",
      "         -2.9020e-01,  4.6307e-01, -1.3914e-01,  2.2685e-01,  9.2788e-04,\n",
      "         -3.1509e-01, -3.4732e-02,  4.3706e-01,  3.0095e-02,  8.8117e-02,\n",
      "          1.0509e-01,  8.2282e-01,  4.5356e-02, -9.2058e-02, -6.2109e-01,\n",
      "          4.2105e-01, -6.8110e-02, -3.5922e-01,  8.4103e-01, -6.5034e-02,\n",
      "         -7.7462e-02,  3.4038e-01, -3.6672e-01,  5.7344e-01,  5.7258e-01,\n",
      "         -6.9085e-01, -2.1262e-01, -3.4172e-01, -5.1187e-02,  9.2013e-01,\n",
      "          4.8508e-01,  1.8545e-01, -1.9722e-01,  3.3566e-01,  3.1268e-01,\n",
      "          1.8657e-01, -1.0697e-01,  1.9067e-01,  1.7733e-01,  1.0186e-01,\n",
      "         -2.1202e-01,  3.0115e-01, -7.4549e-01, -1.1937e+00,  1.1067e+00,\n",
      "         -1.0942e-01, -5.3185e-01, -2.5366e-01, -1.6627e-01, -4.8729e-02,\n",
      "         -7.2283e-01, -4.1233e-02,  5.9362e-02, -6.2387e-01, -7.5335e-02,\n",
      "          6.2291e-01,  3.9084e-01, -1.4686e-01, -2.5707e-01, -4.7694e-01,\n",
      "         -4.7276e-01, -1.6142e-01, -2.8780e-01,  6.0136e-01,  5.1123e-02,\n",
      "         -6.0326e-01,  4.8704e-01, -6.0836e-01, -5.9666e-01,  1.4270e-01,\n",
      "          8.8341e-01, -2.6950e-01, -3.3544e-01,  4.2611e-01, -6.3247e-02,\n",
      "          6.9633e-02,  4.8337e-01, -2.3570e-01, -1.0081e+00, -5.5894e-01,\n",
      "         -3.7488e-01,  3.6223e-01,  6.6310e-01, -2.0714e-01,  2.3993e-01,\n",
      "         -2.5802e-01, -4.0611e-01, -9.5164e-02, -4.1973e-01, -3.6559e-01,\n",
      "         -4.6916e-01,  6.5247e-01,  3.5503e-01, -3.4600e-01, -5.3361e-01,\n",
      "          9.2945e-01, -4.0606e-02,  9.5934e-01, -8.2439e-02,  2.9774e-01,\n",
      "         -2.5744e-01,  8.0028e-01,  2.0915e-01, -5.4336e-01,  1.3325e-01,\n",
      "          8.5542e-02, -5.4832e-02,  6.8877e-01, -5.6369e-01, -9.5480e-03,\n",
      "         -6.9496e-02, -3.8476e-02, -9.1362e-02,  5.9586e-01, -1.6419e-01,\n",
      "         -2.7076e-01,  2.2572e-01, -6.0100e-01, -5.5767e-01,  8.0793e-01,\n",
      "         -7.2821e-02,  1.7871e-01,  6.7563e-01]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3183, -0.9978, -0.2387,  0.7496,  0.0292, -0.2880,  0.4519, -0.1361,\n",
      "          0.2128, -0.0033, -0.3100, -0.0278,  0.4137,  0.0097,  0.0740,  0.1019,\n",
      "          0.8188,  0.0543, -0.1045, -0.6172,  0.4069, -0.0622, -0.3504,  0.8419,\n",
      "         -0.0560, -0.0849,  0.3311, -0.3566,  0.5627,  0.5600, -0.6806, -0.2006,\n",
      "         -0.3311, -0.0590,  0.9260,  0.4645,  0.1778, -0.1913,  0.3368,  0.3233,\n",
      "          0.1936, -0.1033,  0.1802,  0.1776,  0.1124, -0.2261,  0.3051, -0.7357,\n",
      "         -1.1803,  1.0862, -0.1061, -0.5127, -0.2355, -0.1563, -0.0510, -0.7154,\n",
      "         -0.0506,  0.0782, -0.6227, -0.0825,  0.6152,  0.3801, -0.1533, -0.2697,\n",
      "         -0.4552, -0.4527, -0.1524, -0.2696,  0.6074,  0.0455, -0.6002,  0.4877,\n",
      "         -0.5982, -0.5668,  0.1347,  0.8720, -0.2536, -0.3330,  0.4221, -0.0756,\n",
      "          0.0537,  0.4628, -0.2356, -0.9889, -0.5419, -0.3736,  0.3686,  0.6633,\n",
      "         -0.2156,  0.2430, -0.2502, -0.4017, -0.0943, -0.4140, -0.3658, -0.4698,\n",
      "          0.6541,  0.3549, -0.3263, -0.5298,  0.9218, -0.0388,  0.9514, -0.0830,\n",
      "          0.2934, -0.2543,  0.7929,  0.2090, -0.5509,  0.1159,  0.0858, -0.0578,\n",
      "          0.6831, -0.5598, -0.0165, -0.0731, -0.0441, -0.0981,  0.5873, -0.1627,\n",
      "         -0.2720,  0.2190, -0.6000, -0.5462,  0.7949, -0.0654,  0.1873,  0.6832]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3139, -0.9927, -0.2350,  0.7555,  0.0375, -0.2891,  0.4609, -0.1359,\n",
      "          0.2156, -0.0042, -0.3151, -0.0284,  0.4260,  0.0105,  0.0664,  0.1101,\n",
      "          0.8134,  0.0526, -0.1025, -0.6109,  0.4153, -0.0653, -0.3453,  0.8368,\n",
      "         -0.0542, -0.0842,  0.3205, -0.3552,  0.5618,  0.5579, -0.6846, -0.1965,\n",
      "         -0.3372, -0.0604,  0.9274,  0.4707,  0.1738, -0.2033,  0.3391,  0.3148,\n",
      "          0.1914, -0.1151,  0.1913,  0.1584,  0.1169, -0.2249,  0.3006, -0.7311,\n",
      "         -1.1788,  1.0872, -0.1022, -0.5093, -0.2334, -0.1630, -0.0425, -0.7169,\n",
      "         -0.0416,  0.0698, -0.6232, -0.0742,  0.6123,  0.3903, -0.1501, -0.2635,\n",
      "         -0.4647, -0.4583, -0.1585, -0.2730,  0.6154,  0.0500, -0.5930,  0.4849,\n",
      "         -0.5958, -0.5760,  0.1309,  0.8681, -0.2550, -0.3314,  0.4254, -0.0673,\n",
      "          0.0566,  0.4597, -0.2335, -0.9923, -0.5384, -0.3778,  0.3669,  0.6652,\n",
      "         -0.2055,  0.2412, -0.2501, -0.4066, -0.0864, -0.4260, -0.3692, -0.4676,\n",
      "          0.6525,  0.3455, -0.3329, -0.5289,  0.9336, -0.0437,  0.9528, -0.0858,\n",
      "          0.2967, -0.2507,  0.7847,  0.2164, -0.5536,  0.1113,  0.0879, -0.0584,\n",
      "          0.6817, -0.5606, -0.0115, -0.0675, -0.0403, -0.0893,  0.5952, -0.1624,\n",
      "         -0.2719,  0.2266, -0.5891, -0.5392,  0.8009, -0.0672,  0.1789,  0.6848]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3309, -1.0030, -0.2407,  0.7634,  0.0319, -0.2918,  0.4609, -0.1402,\n",
      "          0.2242,  0.0036, -0.3202, -0.0335,  0.4186,  0.0206,  0.0766,  0.1009,\n",
      "          0.8267,  0.0572, -0.1134, -0.6215,  0.4149, -0.0645, -0.3512,  0.8555,\n",
      "         -0.0593, -0.0808,  0.3364, -0.3650,  0.5617,  0.5680, -0.6862, -0.2141,\n",
      "         -0.3322, -0.0619,  0.9317,  0.4785,  0.1769, -0.2002,  0.3325,  0.3293,\n",
      "          0.1821, -0.1069,  0.1901,  0.1715,  0.1142, -0.2252,  0.3121, -0.7462,\n",
      "         -1.1878,  1.1054, -0.1103, -0.5188, -0.2405, -0.1551, -0.0483, -0.7206,\n",
      "         -0.0414,  0.0707, -0.6220, -0.0707,  0.6210,  0.3883, -0.1574, -0.2762,\n",
      "         -0.4633, -0.4658, -0.1564, -0.2733,  0.6146,  0.0593, -0.6053,  0.4893,\n",
      "         -0.6016, -0.5861,  0.1330,  0.8801, -0.2654, -0.3314,  0.4332, -0.0765,\n",
      "          0.0617,  0.4673, -0.2379, -1.0010, -0.5559, -0.3855,  0.3712,  0.6620,\n",
      "         -0.2061,  0.2459, -0.2500, -0.4058, -0.1038, -0.4189, -0.3705, -0.4718,\n",
      "          0.6573,  0.3516, -0.3300, -0.5339,  0.9278, -0.0399,  0.9608, -0.0920,\n",
      "          0.2982, -0.2557,  0.7928,  0.2074, -0.5525,  0.1199,  0.0890, -0.0607,\n",
      "          0.6919, -0.5639, -0.0204, -0.0722, -0.0449, -0.0967,  0.5897, -0.1638,\n",
      "         -0.2735,  0.2255, -0.6075, -0.5550,  0.8067, -0.0648,  0.1879,  0.6921]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for subset in dataset.datasets:\n",
    "    print(siamese_model(dataset.datasets[subset][0][0].reshape(1,3,224,224)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
